---
title: "Web_proj"
author: "Xueting Wang"
date: "5/17/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Part 1. Data Cleaning

```{r}
library('data.table') # data manipulation
library('dplyr') # data manipulation
library('readr') # data input
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('stringr') # string manipulation
library('purrr') # string manipulation
library('forcats') # factor manipulation
```

```{r message=FALSE, warning=FALSE, echo = FALSE, results=FALSE}
train <- read.csv("/Users/wxt/Desktop/ts_proj/data/train_2.csv", header=T)
head(train)
```

### Drop Any websites with null values (From 145,063 to 117,277)
```{r}
library(tidyr)
train <- train %>% drop_na()
c(ncol(train),nrow(train))
```

### Keep website with domain wikipedia.org
```{r}

foo <- train %>% rownames_to_column()
wikipedia <- foo %>% filter(str_detect(Page, "wikipedia.org")) %>% 
  filter(!str_detect(Page, "wikimedia")) %>%
  filter(!str_detect(Page, "mediawiki"))

wikipedia <- wikipedia[-1]
head(wikipedia)

```

### Select all websites relevant to The Big Bang Theory
```{r}

foo <- wikipedia %>% rownames_to_column()

strings <- c("The_Big_Bang_Theory")

films <- foo %>% 
  filter(str_detect(Page, paste(strings, collapse = "|")))

films <- films[-1]
films <- films[-c(3),]
films

```

### Calculate the mean of each page to find the largest views
```{r}
mean <- data.frame(Page = films$Page, Mean = rowMeans(films[-1]))
mean_top <- mean[order(-mean$Mean),]
mean_top
```

### Convert character Dates to Datetime and clean the dataframe
```{r}
library(dplyr)
library(lubridate)
films.T <- data.frame(t(films))
colnames(films.T) <- films$Page
films.T <- data.frame(Date = colnames(films), films.T)
rownames(films.T) <- NULL
films.T <- films.T[-1, ]
films.T$Date = sub('X', '', films.T$Date)
films.T <- films.T %>% mutate(Date = ymd(Date))
films.T <- films.T %>% relocate(The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents, .after = Date)
films.T
```


```{r}
#write.csv(films.T, "/Users/wxt/Desktop/final_ts.csv")
```


Part 2. Exploratory Data Analysis

### Import data and packages

```{r, warning=FALSE, message=FALSE}
# Loading
library(xts)
library(timetk)
library("tseries")
library(forecast)
library(TSA)
library(car)
library(lmtest)
library(fGarch)
library(rugarch)
library(prophet)
```


```{r}
suppressMessages(library(rvest))
suppressMessages(library(tidyverse))
suppressMessages(library(lubridate))
suppressMessages(library(prophet))
suppressMessages(library(forecast))
suppressMessages(library(data.table))
suppressMessages(library(zoo))
```


```{r}
df <- read.csv("/Users/wxt/Desktop/ts_proj/final_ts.csv")

df$Date <- as.Date(df$Date,)
```

### Data properties (stationarity, correlations, data distribution)

```{r}
ts1 <- xts(df$The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents, order.by=df$Date)
```

#### Plot the time series

```{r}
Sys.setlocale(locale = "English")
```


```{r}
plot(ts1, main = "The_Big_Bang_Theory")
```

#### Find and clean the outliers

```{r}
tsoutliers(ts1)
ts1_clean <- tsclean(ts1)
plot(ts1, main = "The_Big_Bang_Theory")
lines(ts1_clean, col='red')
```


#### Plot the distribution of data

```{r}
plot_density <- function(data, title) {
  hist(data, # histogram
       col="peachpuff", # column color
       border="black",
       prob = TRUE, # show densities instead of frequencies
       xlab = "views",#ylim = c(0, max(ts1)),
       main = title)
  lines(density(data), # density plot
        lwd = 2, # thickness of line
        col = "chocolate3")
}
```

```{r}
plot_density(ts1, "The_Big_Bang_Theory")
```


#### Plot acf for each ts
```{r}
acf(ts1, main = "The_Big_Bang_Theory")
```


#### Perform ADF and KPSS test
```{r}
# non-stationary - result match
adf.test(ts1)
kpss.test(ts1)
```



Part 3. Models

### Fit models

#### prophet

```{r}
data <- data.frame(ds = df$Date, y=df$The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents)
```

```{r}
data_clean <- data.frame(date=index(ts1_clean), coredata(ts1_clean))
colnames(data_clean) <- c("ds", "y")
```

```{r}
lam <- BoxCox.lambda(data_clean$y, method = "loglik")
data_clean$y_new <- BoxCox(data_clean$y, lam)
df <- data.frame(ds = data_clean$ds, y = data_clean$y_new)
```

```{r}
train <- df[df$ds <= "2017-07-10",]
test  <- df[df$ds >  "2017-07-10",]
```


```{r}
acc   <- c()
out.m <- out.forecast <- list()
m  <- prophet(df = train[,c("ds","y")], yearly.seasonality = TRUE)
  
# get no. of days for forecasting.
forecastDays <- difftime(as.Date("2017-09-10"), as.Date("2017-07-10"))
future       <- make_future_dataframe(m, periods = forecastDays)
forecast     <- predict(m, future)

```


```{r}
a1           <- inner_join(forecast, test, by = "ds") %>%  select(ds, yhat, y)
acc[1]       <- accuracy(object = a1$yhat, x = a1$y)[2] #RMSE
  
out.m[[1]]        <- m
out.forecast[[1]] <- forecast
```

```{r}
plot(m, forecast)
#points(tail(forecast, n=31)$y_hat, color="red")
```

```{r}
br   <- seq(ymd(first(train$ds)), ymd("2017-09-10"), by = '3 month')
lbs  <- as.character(br)
```

```{r}
options(repr.plot.width = 20, repr.plot.height = 10)
ggplot() +
  scale_x_date(breaks = br, labels = as.Date(lbs)) +
  theme(axis.text.x = element_text(angle = 60, hjust =1)) +
  geom_point(data = na.omit(train), aes(x = as.Date(ds,"%Y-%m-%d"), y = y), alpha = .5) +
  geom_point(data = test, aes(x = as.Date(ds,"%Y-%m-%d"), y = y), color = 'red', alpha = .5) + 
  geom_line(data = out.forecast[[1]], aes(x = as.Date(ds,"%Y-%m-%d"), y = yhat), color = "blue") +
  geom_ribbon(data = out.forecast[[1]], aes(x = as.Date(ds,"%Y-%m-%d"), ymin = yhat_lower, ymax = yhat_upper), 
              fill = "blue", alpha = .4) +
  labs(y= "Views", x = "date")
```

```{r}
plot(ts(tail(data, n=62)$y,start=min(data$ds)), xlab = "Time", ylab = "Views")
lines(ts(tail(data, n=62)$y_hat,start=min(data$ds)), col = "red")
```

```{r}
yhat_untransformed <- InvBoxCox(forecast$yhat, lam)
data$y_hat <- yhat_untransformed
```

```{r}
# errors for untransformed y train - original
accuracy(head(data, n=741)$y_hat, head(data, n=741)$y)
```


```{r}
# errors for untransformed y test - original
accuracy(tail(data, n=61)$y_hat, tail(data, n=61)$y)
```
```{r}
# errors for transformed y
accuracy(a1$yhat, a1$y)
```


```{r}
prophet_plot_components(out.m[[1]], out.forecast[[1]])
```




#### Seasonal decomposition
```{r}
library(xts)
train <- read.csv("final_ts.csv", header=T)
df <-  train[2:3]
df$Date <- as.Date(df$Date)
ts1 <- xts(df$The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents, order.by=df$Date)
ts_new <- ts(df$The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents, start =c(2015-07-01),frequency = 30)
plot(stl(ts_new,s.window = "periodic"))
```

#### Plot weekday average
```{r}
library(tidyverse) 
library(radiant.data)
colnames(train)
w1<- train[,c(2,3)] %>%
  mutate(Date = wday(Date, label = TRUE)) %>%
  group_by(Date) %>%
  summarise(wday_views = mean(The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = "English")

w2<- train[,c(2,8)] %>%
  mutate(Date = wday(Date, label = TRUE)) %>%
  group_by(Date) %>%
  summarise(wday_views = mean(The_Big_Bang_Theory_fr.wikipedia.org_all.access_all.agents)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = "French")

w3<- train[,c(2,22)] %>%
  mutate(Date = wday(Date, label = TRUE)) %>%
  group_by(Date) %>%
  summarise(wday_views = mean(The_Big_Bang_Theory_es.wikipedia.org_all.access_all.agents)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = "Español")

w4<- train[,c(2,27)] %>%
  mutate(Date = wday(Date, label = TRUE)) %>%
  group_by(Date) %>%
  summarise(wday_views = mean(The_Big_Bang_Theory_de.wikipedia.org_all.access_all.agents)) %>%
  mutate(wday_views = wday_views/mean(wday_views)) %>%
  mutate(id = "Germany")

all_w <- bind_rows(w1,w2,w3,w4)

all_w %>%
  ggplot(aes(Date, wday_views, color = id)) +
  geom_jitter(size = 4, width = 0.1) +
  labs(x = "Day of the week", y = "Relative average views")

```


### Plot all time-series 
```{r}
t1 <- t(train[,3])
t2 <- t(train[,8])
t3 <- t(train[,22])
t4 <- t(train[,27])
thedate <- t(train$Date)
allt <- as.data.frame(rbind(t1,t2,t3,t4))
language <- c("English","French","Español","Germany")
allt <- cbind(language,allt)
colnames(allt) <- thedate
library(ggplot2)
library(reshape2)
meltdf <- melt(allt,id="Date")



temp <- melt(allt,id.vars=c("language"),value.name="value",
                     variable.name="Day")


plot <- ggplot(data=temp, aes(x=as.Date(Day), y=value, group = language, colour = language)) + geom_line() +labs(y= "View", x = "Day")
plot + ggtitle("Webpage trafic for different languages")+geom_point()

plot

```


### ARIMA
```{r}
library(forecast)
ts_1 <- xts(df$The_Big_Bang_Theory_en.wikipedia.org_all.access_all.agents, order.by=df$Date)

temp <- ts_1["/2017-07-10"]
train <- tsclean(temp)
test <- ts_1["2017-07-11/"]

train_ts <- ts(train,start=start(train),frequency = 7)
arima <- auto.arima(train_ts, seasonal = TRUE)
pre <- forecast(arima, h=62)
plot(pre)

summary(arima)
accuracy(pre,test)

plot(train_ts,col="black")
lines(arima$fitted,col="red")
acf(arima$residuals)

```

```{R,warning = False}
library(tseries)
diff = diff(ts1)[2:803,]
adf.test(diff)
kpss.test(diff)
```

```{R,warning = False}
library(forecast)
t2 <- ts(ts_new[1:741],start = 0, frequency = 7)
#decomp <- stl(t2, s.window='periodic') 
tsmod <- stlm(t2, method = "arima")
pre = forecast(tsmod, h = 62)
accuracy(pre,test)
plot(pre)




plot(t2,col="black")
lines(tsmod$fitted,col="red")
```
